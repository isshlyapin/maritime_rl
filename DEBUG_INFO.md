# Добавленная отладочная информация

## Изменения в коде для диагностики проблем с обучением

### 1. Расширенная информация об обучении нейросети (`update_model`)

Теперь метод возвращает словарь с детальной отладочной информацией:
- **loss** - значение функции потерь
- **grad_norm** - норма градиентов (помогает обнаружить проблемы с взрывающимися/затухающими градиентами)
- **q_mean, q_std, q_max, q_min** - статистика Q-значений (показывает, обучается ли сеть)
- **target_q_mean** - среднее целевое Q-значение
- **reward_mean, reward_std** - статистика наград в батче

### 2. Детализация компонентов награды (`_calculate_reward`)

При включенном режиме `verbose` выводятся:
- **r_ca** - награда за избежание столкновений
- **r_ne** - награда за эффективность навигации
- **r_ce** - награда за соответствие правилам (COLREGs)
- Текущее состояние корабля (скорость, курс, расстояние до цели)
- Отклонения от желаемых параметров

### 3. Отслеживание состояний (`_get_state`)

При `verbose=True` выводится:
- Сырые значения первых 7 измерений состояния
- Диапазон значений состояния (min/max)
- Среднее и стандартное отклонение

### 4. Информация о шагах симуляции (`step`)

Добавлен возврат **termination_reason** с причинами завершения эпизода:
- `"collision"` - столкновение
- `"goal_reached"` - цель достигнута
- `"max_steps"` - превышено максимальное количество шагов

### 5. Детальное логирование эпизодов (`train_model`)

Для каждого эпизода выводится:
- Общая статистика: количество шагов, причина завершения
- Награды: текущая и средняя за последние 100 эпизодов
- Метрики обучения: loss, норма градиентов, среднее Q-значение
- Состояние replay buffer: текущий размер / максимальный размер
- Частота столкновений
- Распределение действий (для verbose-режима)
- Статистика наград внутри эпизода (min/max/mean)

### 6. Улучшенная инициализация (`train_model`)

При запуске выводится:
- Размерность пространства состояний
- Размерность пространства действий
- Количество кораблей
- Количество ближайших кораблей для учета

## Как использовать

### Обычный режим
Просто запустите `python main.py` - получите стандартный вывод для каждого эпизода.

### Детальная отладка первых эпизодов
Первые 5 эпизодов и каждый 100-й эпизод автоматически работают в verbose-режиме, выводя подробную информацию о:
- Инициализации окружения
- Первых 10 шагах каждого эпизода
- Компонентах наград
- Состояниях
- Обновлениях нейросети

## На что обращать внимание

### Проблемы с обучением могут быть связаны с:

1. **Градиенты**
   - Если `Grad` очень маленький (< 0.0001) - затухающие градиенты
   - Если `Grad` очень большой (> 10) - взрывающиеся градиенты

2. **Q-значения**
   - Если `Q-mean` не меняется - сеть не обучается
   - Если `Q-mean` уходит в NaN или Inf - проблемы со стабильностью

3. **Loss**
   - Если `Loss` не уменьшается - проблемы с оптимизацией
   - Если `Loss` резко колеблется - возможно, слишком большой learning rate

4. **Награды**
   - Проверьте баланс между r_ca, r_ne, r_ce
   - Убедитесь, что награды не всегда одинаковые
   - Проверьте, что штрафы и бонусы применяются корректно

5. **Replay Buffer**
   - Убедитесь, что буфер заполняется (Buffer size растет)
   - Проверьте, что обучение начинается только после накопления достаточного опыта

6. **Действия**
   - Проверьте распределение действий - агент не должен выбирать всегда одно и то же
   - Убедитесь, что epsilon уменьшается и агент переходит от exploration к exploitation

7. **Состояния**
   - Убедитесь, что значения состояний в разумных пределах (не NaN, не Inf)
   - Проверьте, что нормализация работает корректно (значения примерно в диапазоне [-2, 2])
