# Краткое руководство по отладке

## Быстрый старт

1. Запустите обучение:
   ```bash
   python main.py
   ```

2. Обратите внимание на вывод первых эпизодов - они содержат детальную информацию.

## Ключевые метрики при запуске

### Каждый эпизод выводит:

```
Episode 0/10000 | Steps: 156 | Reason: max_steps
  Reward: -78.45 (avg: -78.45) | Epsilon: 0.998
  Loss: 0.1234 | Grad: 0.4567 | Q-mean: -2.3456
  Buffer: 156/100000 | Collisions: 0.000
```

**Объяснение:**
- `Steps` - сколько шагов прошел агент
- `Reason` - почему эпизод завершился (max_steps/collision/goal_reached)
- `Reward` - награда за эпизод и средняя за последние 100
- `Epsilon` - текущее значение epsilon для epsilon-greedy стратегии
- `Loss` - функция потерь (должна постепенно уменьшаться)
- `Grad` - норма градиентов (обычно 0.1-2.0)
- `Q-mean` - среднее Q-значение (должно меняться со временем)
- `Buffer` - размер replay buffer
- `Collisions` - средняя частота столкновений

## Признаки проблем

### 1. Модель не учится совсем
**Симптомы:**
- Q-mean не меняется от эпизода к эпизоду
- Loss остается константой
- Reward не улучшается

**Возможные причины:**
- Награды слишком маленькие или всегда нулевые (проверьте компоненты r_ca, r_ne, r_ce)
- Learning rate слишком маленький
- Градиенты затухают (Grad < 0.0001)

### 2. Нестабильное обучение
**Симптомы:**
- Loss резко колеблется
- Q-values уходят в Inf или NaN
- Grad > 10

**Возможные причины:**
- Learning rate слишком большой
- Проблемы с нормализацией состояний
- Взрывающиеся градиенты

### 3. Агент не исследует среду
**Симптомы:**
- Всегда выбирает одни и те же действия
- Epsilon быстро падает до минимума
- Buffer заполняется похожими переходами

**Возможные причины:**
- Epsilon decay слишком быстрый
- Награды сильно смещены в сторону одной стратегии

### 4. Не достигает цели
**Симптомы:**
- Reason всегда "max_steps" или "collision"
- Никогда не появляется "goal_reached"

**Возможные причины:**
- Цель слишком далеко (проверьте начальное расстояние в логах)
- Награда за навигацию (r_ne) слишком слабая
- Агент слишком фокусируется на избежании столкновений

## Что делать, если модель не учится

### Шаг 1: Проверьте награды
Посмотрите на вывод первых эпизодов:
```
[REWARD] r_ca=0.000, r_ne=-0.350, r_ce=0.000, total=-0.140
```

- Проверьте, что все компоненты активны
- Убедитесь, что награды в разумном диапазоне (-1 до 1 обычно)
- Проверьте баланс между компонентами

### Шаг 2: Проверьте состояния
```
[STATE] State range: [-0.856, 1.234]
```

- Значения должны быть конечными (не NaN, не Inf)
- Обычно в диапазоне [-2, 2] для нормализованных значений
- Проверьте, что состояния меняются от шага к шагу

### Шаг 3: Проверьте обучение
- `Loss` должен в целом уменьшаться (могут быть колебания)
- `Q-mean` должен меняться и стабилизироваться со временем
- `Grad` должен быть в диапазоне 0.1-2.0

### Шаг 4: Проверьте гиперпараметры
В `train_model()` попробуйте изменить:
```python
lr=0.0005,           # Уменьшите до 0.0001 если нестабильно
gamma=0.99,          # Discount factor
batch_size=1024,     # Уменьшите до 64-256 если не хватает памяти
target_update=1000,  # Как часто обновлять target network
```

В расчете награды попробуйте изменить веса:
```python
alpha, beta, gamma = 0.4, 0.4, 0.2  # Веса для r_ca, r_ne, r_ce
```

## Сохранение моделей

Модели автоматически сохраняются каждые 10 эпизодов в папку `models/`:
```
models/ship_collision_avoidance_model0.pth
models/ship_collision_avoidance_model10.pth
models/ship_collision_avoidance_model20.pth
...
```

Вы можете загрузить модель для тестирования:
```python
agent.target_net.load_state_dict(torch.load("models/ship_collision_avoidance_model1000.pth"))
```

## Дополнительная информация

Подробное описание всех изменений и метрик см. в файле `DEBUG_INFO.md`.
